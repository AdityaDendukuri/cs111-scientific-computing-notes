\documentclass[../readings.tex]{subfiles}

\begin{document}

\subsection{Symmetric Positive Definite (SPD) Matrices}

\addDef{Symmetric Matrix}{%
A matrix \(\bA \in \mathbb{R}^{n \times n}\) is called \emph{symmetric} if 
\[
\bA = \bA^T,
\]
meaning that the entry in the \(i\)th row and \(j\)th column is the same as the entry in the \(j\)th row and \(i\)th column for all \(1 \le i,j \le n\).
}

\addDef{Positive Definite Matrix}{%
A symmetric matrix \(\bA \in \mathbb{R}^{n \times n}\) is \emph{positive definite} (PD) if
\[
\bx^T\bA\bx > 0 \quad \text{for every nonzero } \bx \in \mathbb{R}^n.
\]
In computational terms, when we transform any nonzero vector \(\bx\) using the quadratic form \(\bx^T\bA\bx\), the result is always a positive number.
}

\addDef{Metric Induced by an SPD Matrix}{%
Let \(\bA \in \mathbb{R}^{n \times n}\) be an SPD matrix. Then, the function
\[
d_\bA(\bx,\by) = \sqrt{ (\bx-\by)^T \bA (\bx-\by) }
\]
defines a distance function on \(\mathbb{R}^n\). This generalizes the standard Euclidean distance ((in this case, x$\bA = \bI$)) by incorporating the transformation defined by \(\bA\). It satisfies all the properties of a metric:
\begin{enumerate}
    \item \textbf{Non-negativity:} \(d_\bA(\bx,\by) \geq 0\) for all \(\bx, \by \in \mathbb{R}^n\).
    \item \textbf{Zero distance means equality:} \(d_\bA(\bx,\by) = 0\) if and only if \(\bx = \by\).
    \item \textbf{Symmetry:} \(d_{\bA}(\bx,\by) = d_{\bA}(\by,\bx)\) for all \(\bx, \by \in \mathbb{R}^n\).
    \item \textbf{Triangle Inequality:} \(d_\bA(\bx,\bz) \leq d_\bA(\bx,\by) + d_\bA(\by,\bz)\) for all \(\bx,\by,\bz \in \mathbb{R}^n\).
\end{enumerate}
This matrix-based distance function is particularly useful in scientific computing applications like clustering, optimization, and data analysis where standard Euclidean distance might not capture the right relationships between data points.
}

\addDef{SPD Interpratation}{%
An SPD matrix transforms vector spaces by rescaling their components in a way that preserves a consistent notion of size. In standard Euclidean space, the norm \(\|\bx\|\) returns a positive length for any nonzero vector \(\bx\). Similarly, an SPD matrix \(\bA\) creates a new norm 
\[
\|\bx\|_{\bA} = \sqrt{\bx^T\bA\bx},
\]
which remains strictly positive for all nonzero \(\bx\). This property ensures that the transformation never collapses vectors to zero length or produces negative squared lengths, which is crucial for numerical stability in computational algorithms.
}


\addNote{%
Another way to visualize SPD matrices is through their geometric interpretation. When you use an SPD matrix \(\bA\) to compute distances via \(\bx^T\bA\bx\), the set of all vectors with constant "size" forms an ellipsoid (a stretched or squashed sphere). This is important in scientific computing applications like principal component analysis (PCA), where we often need to understand how data varies in different directions. The axes of this ellipsoid correspond to the eigenvectors of \(\bA\), and their lengths are determined by the eigenvalues.
}
\addDef{Key Properties of SPD Matrices}{%
Let \(\bA \in \mathbb{R}^{n \times n}\) be an SPD matrix. Then:
\begin{enumerate}
    \item Every eigenvalue \(\lambda\) of \(\bA\) satisfies \(\lambda > 0\).
    \item \(\bA\) is invertible, meaning it has full rank and its determinant is non-zero.
    \item The function \(\langle \bx, \by \rangle_\bA = \bx^T\bA\by\) defines an inner product, which induces the norm \(\|\bx\|_\bA = \sqrt{\bx^T\bA\bx}\). This gives us a computationally consistent way to calculate distances in transformed spaces.
\end{enumerate}
}

\textBox{%
From a computational perspective, the condition \(\bx^T\bA\bx > 0\) for all nonzero \(\bx\) ensures that the matrix \(\bA\) never maps a vector to a zero result through the quadratic form, nor does it produce negative values. This is critical in numerical algorithms because it guarantees the stability and uniqueness of solutions. For example, when solving linear systems like \(\bA\bx = \bb\) where \(\bA\) is SPD, efficient methods like Cholesky decomposition can be used, and the solution is guaranteed to exist and be unique.
}

\addNote{%
SPD matrices are fundamental in scientific computing applications. For example:
\begin{itemize}
    \item In machine learning, covariance matrices are SPD and used to understand data distributions
    \item In numerical optimization, the Hessian matrix of a convex function is SPD near the minimum
    \item In finite element methods, the stiffness matrix is typically SPD
    \item In numerical linear algebra, SPD matrices allow for specialized algorithms like Cholesky factorization, which is twice as efficient as LU decomposition
    \item In image processing, SPD matrices can represent local structure tensors that capture directional information
\end{itemize}
Their computational properties make algorithms more stable, efficient, and reliable.
}

\subsubsection{Cholesky Decomposition}

\addDef{Cholesky Decomposition}{%
For a symmetric positive definite (SPD) matrix \(\bA \in \mathbb{R}^{n \times n}\), the Cholesky Decomposition expresses \(\bA\) as 
\[
\bA = \bL\bL^T,
\]
where \(\bL\) is a unique lower triangular matrix with strictly positive diagonal entries.
}

\textBox{%
SPD matrices, which satisfy \(\bx^T\bA\bx > 0\) for all nonzero \(\bx\), are prevalent in optimization, statistics, and finite element methods. The Cholesky factorization is both computationally efficient and numerically stable since it avoids the need for pivoting.
}

\addDef{Algorithm for Cholesky Decomposition}{%
For an SPD matrix \(\bA \in \mathbb{R}^{n \times n}\), the Cholesky factorization \(\bA = \bL\bL^T\) can be computed entry by entry as follows:

For each entry \(l_{ij}\) of \(\bL\) where \(1 \leq i,j \leq n\):
\begin{align}
l_{ij} &= 
\begin{cases}
0, & \text{if } i < j \\
\sqrt{a_{ii} - \sum_{k=1}^{i-1} l_{ik}^2}, & \text{if } i = j \\
\frac{1}{l_{jj}} \left( a_{ij} - \sum_{k=1}^{j-1} l_{ik}l_{jk} \right), & \text{if } i > j
\end{cases}
\end{align}

This algorithm proceeds row by row, using previously computed elements of \(\bL\) to determine subsequent entries.
}

\textBox{%
\textbf{Derivation of the Cholesky Algorithm (Matrix Form)}

To derive the Cholesky algorithm, we start with the relationship $\bA = \bL\bL^T$ where $\bA$ is an SPD matrix and $\bL$ is lower triangular. Let's express both matrices explicitly:

$$
\bA = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}, \quad
\bL = \begin{pmatrix}
l_{11} & 0 & \cdots & 0 \\
l_{21} & l_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{n1} & l_{n2} & \cdots & l_{nn}
\end{pmatrix}
$$

The transpose of $\bL$ is:

$$
\bL^T = \begin{pmatrix}
l_{11} & l_{21} & \cdots & l_{n1} \\
0 & l_{22} & \cdots & l_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & l_{nn}
\end{pmatrix}
$$

Now, the matrix product $\bL\bL^T$ gives us:

$$
\bL\bL^T = \begin{pmatrix}
l_{11}^2 & l_{11}l_{21} & \cdots & l_{11}l_{n1} \\
l_{21}l_{11} & l_{21}^2 + l_{22}^2 & \cdots & l_{21}l_{n1} + l_{22}l_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
l_{n1}l_{11} & l_{n1}l_{21} + l_{n2}l_{22} & \cdots & \sum_{k=1}^{n} l_{nk}^2
\end{pmatrix}
$$

Equating this with $\bA$, we can derive the formulas for different elements of $\bL$:

\textbf{Case 1: Diagonal elements (position $(i,i)$)}

From the diagonal entries of $\bA = \bL\bL^T$, we have:
$$
a_{ii} = \sum_{k=1}^{i} l_{ik}^2 = l_{i1}^2 + l_{i2}^2 + \cdots + l_{ii}^2
$$

Solving for $l_{ii}$:
$$
l_{ii} = \sqrt{a_{ii} - \sum_{k=1}^{i-1} l_{ik}^2}
$$

\textbf{Case 2: Below diagonal (position $(i,j)$ where $i > j$)}

From the below-diagonal entries of $\bA = \bL\bL^T$, we have:
$$
a_{ij} = \sum_{k=1}^{j} l_{ik}l_{jk} = l_{i1}l_{j1} + l_{i2}l_{j2} + \cdots + l_{ij}l_{jj}
$$

Since $l_{jk} = 0$ for $k > j$ (as $\bL$ is lower triangular), the sum stops at $k = j$. 
Solving for $l_{ij}$:
$$
l_{ij} = \frac{1}{l_{jj}}\left(a_{ij} - \sum_{k=1}^{j-1} l_{ik}l_{jk}\right)
$$

\textbf{Case 3: Above diagonal (position $(i,j)$ where $i < j$)}

Since $\bA$ is symmetric, $a_{ij} = a_{ji}$. The elements of $\bL$ above the diagonal are defined to be zero: $l_{ij} = 0$ for $i < j$.

This gives us the complete algorithm for computing the Cholesky decomposition:
$$
l_{ij} = 
\begin{cases}
0, & \text{if } i < j \\
\sqrt{a_{ii} - \sum_{k=1}^{i-1} l_{ik}^2}, & \text{if } i = j \\
\frac{1}{l_{jj}} \left( a_{ij} - \sum_{k=1}^{j-1} l_{ik}l_{jk} \right), & \text{if } i > j
\end{cases}
$$

The algorithm proceeds by computing $\bL$ one column at a time, from left to right, and within each column from top to bottom.
}

\addDef{Computational Complexity (Cholesky)}{%
The Cholesky decomposition algorithm requires:
\begin{itemize}
    \item Approximately \(\frac{1}{3}n^3\) floating-point operations (flops) for an \(n \times n\) matrix
    \item This is roughly half the cost of LU factorization (\(\frac{2}{3}n^3\) flops), which is the standard method for non-symmetric matrices
    \item Storage requirement of \(\frac{n(n+1)}{2}\) elements (only the lower triangular part is stored)
\end{itemize}
The efficiency comes from exploiting both the symmetry of \(\bA\) and its positive definiteness, which eliminates the need for pivoting strategies.
}

\addExmpl{%
Let's compute the Cholesky decomposition of:
\[
\bA = \begin{pmatrix} 4 & 2 \\ 2 & 3 \end{pmatrix}
\]

Step 1: Compute the first column of \(\bL\).
\begin{align}
l_{11} &= \sqrt{a_{11}} = \sqrt{4} = 2 \\
l_{21} &= \frac{a_{21}}{l_{11}} = \frac{2}{2} = 1
\end{align}

Step 2: Compute the second column of \(\bL\).
\begin{align}
l_{22} &= \sqrt{a_{22} - l_{21}^2} = \sqrt{3 - 1^2} = \sqrt{2} \approx 1.414
\end{align}

Therefore:
\[
\bL = \begin{pmatrix} 2 & 0 \\ 1 & \sqrt{2} \end{pmatrix}
\]

Verification: 
\[
\bL\bL^T = \begin{pmatrix} 2 & 0 \\ 1 & \sqrt{2} \end{pmatrix} \begin{pmatrix} 2 & 1 \\ 0 & \sqrt{2} \end{pmatrix} = \begin{pmatrix} 4 & 2 \\ 2 & 3 \end{pmatrix} = \bA
\]
}

\addNote{%
The uniqueness of the Cholesky Decomposition stems from the positivity of the leading principal minors of an SPD matrix, ensuring that each pivot in the elimination process is nonzero. For an SPD matrix \(\bA\), the leading principal minor of order \(k\), denoted as \(\det(\bA_k)\), is the determinant of the submatrix formed by the first \(k\) rows and columns. These determinants are all positive, which ensures that the diagonal entries of \(\bL\) are well-defined and unique.
}







\end{document}