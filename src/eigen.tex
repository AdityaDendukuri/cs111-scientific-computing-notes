\documentclass[../readings.tex]{subfiles}


\begin{document}
\subsection{Eigenvalues and Eigenvectors}

\addDef{Eigenvalues and Eigenvectors}{%
For a matrix \(\bA \in \mathbb{R}^{n \times n}\), a scalar \(\lambda \in \mathbb{C}\) is called an \emph{eigenvalue} of \(\bA\) if there exists a nonzero vector \(\bv \in \mathbb{C}^n\) such that
\[
\bA\bv = \lambda\bv.
\]
The nonzero vector \(\bv\) is called an \emph{eigenvector} of \(\bA\) corresponding to the eigenvalue \(\lambda\). The set of all eigenvalues of \(\bA\) is called the \emph{spectrum} of \(\bA\), denoted by \(\sigma(\bA)\).
}

\textBox{%
Intuitively, an eigenvector is a vector whose direction remains unchanged when transformed by the matrix \(\bA\)â€”it may be stretched, compressed, or reversed, but it stays on the same line. The eigenvalue tells us the factor by which the eigenvector is scaled. Eigenvalues and eigenvectors reveal the fundamental character of a linear transformation, showing the directions in which the transformation behaves most simply.
}

\addDef{Characteristic Polynomial}{%
For a matrix \(\bA \in \mathbb{R}^{n \times n}\), the \emph{characteristic polynomial} is defined as
\[
p_{\bA}(\lambda) = \det(\bA - \lambda \bI),
\]
where \(\bI\) is the \(n \times n\) identity matrix. The equation
\[
\det(\bA - \lambda \bI) = 0
\]
is called the \emph{characteristic equation} of \(\bA\). The roots of the characteristic polynomial are precisely the eigenvalues of \(\bA\).
}

\addExmpl{%
Consider the matrix 
\[
\bA = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix}.
\]

To find its eigenvalues, we compute the characteristic polynomial:
\begin{align}
p_{\bA}(\lambda) &= \det(\bA - \lambda \bI) \\
&= \det\begin{pmatrix} 3-\lambda & 1 \\ 1 & 3-\lambda \end{pmatrix} \\
&= (3-\lambda)(3-\lambda) - 1 \cdot 1 \\
&= (3-\lambda)^2 - 1 \\
&= 9 - 6\lambda + \lambda^2 - 1 \\
&= \lambda^2 - 6\lambda + 8 \\
&= (\lambda - 4)(\lambda - 2)
\end{align}

Setting $p_{\bA}(\lambda) = 0$, we get the eigenvalues $\lambda_1 = 4$ and $\lambda_2 = 2$.

For $\lambda_1 = 4$, we find the corresponding eigenvector by solving $(\bA - 4\bI)\bv = \mathbf{0}$:
\begin{align}
\begin{pmatrix} 3-4 & 1 \\ 1 & 3-4 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} &= \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
\begin{pmatrix} -1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} &= \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\end{align}

This gives us $-v_1 + v_2 = 0$, so $v_1 = v_2$. Any nonzero vector of the form $\bv_1 = (t, t)^T$ for $t \neq 0$ is an eigenvector. A normalized eigenvector would be $\bv_1 = \frac{1}{\sqrt{2}}(1, 1)^T$.

Similarly, for $\lambda_2 = 2$, we solve $(\bA - 2\bI)\bv = \mathbf{0}$:
\begin{align}
\begin{pmatrix} 3-2 & 1 \\ 1 & 3-2 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} &= \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
\begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} &= \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\end{align}

This gives us $v_1 + v_2 = 0$, so $v_1 = -v_2$. Any nonzero vector of the form $\bv_2 = (t, -t)^T$ for $t \neq 0$ is an eigenvector. A normalized eigenvector would be $\bv_2 = \frac{1}{\sqrt{2}}(1, -1)^T$.
}

\addDef{Key Properties of Eigenvalues}{%
For a matrix $\bA \in \mathbb{R}^{n \times n}$:
\begin{enumerate}
    \item The sum of the eigenvalues equals the trace of $\bA$: $\sum_{i=1}^{n} \lambda_i = \text{tr}(\bA)$.
    \item The product of the eigenvalues equals the determinant of $\bA$: $\prod_{i=1}^{n} \lambda_i = \det(\bA)$.
    \item The eigenvalues of $\bA^k$ are $\lambda_i^k$ for $i = 1, 2, \ldots, n$.
    \item The eigenvalues of $\bA^{-1}$ (if $\bA$ is invertible) are $\frac{1}{\lambda_i}$ for $i = 1, 2, \ldots, n$.
    \item If $\bA$ is triangular, its eigenvalues are exactly the diagonal entries.
    \item Similar matrices (i.e., matrices $\bA$ and $\bB$ where $\bB = \bV^{-1}\bA\bV$ for some invertible $\bV$) have the same eigenvalues.
\end{enumerate}
}

\addDef{Diagonalization}{%
A matrix $\bA \in \mathbb{R}^{n \times n}$ is \emph{diagonalizable} if there exists an invertible matrix $\bV$ and a diagonal matrix $\mathbf{\Lambda}$ such that
\[
\bA = \bV\mathbf{\Lambda}\bV^{-1}
\]
where the diagonal entries of $\mathbf{\Lambda}$ are the eigenvalues of $\bA$ and the columns of $\bV$ are the corresponding eigenvectors.
}

\addNote{
A matrix $\bA$ is diagonalizable if and only if there exist $n$ linearly independent eigenvectors, which occurs if and only if the geometric multiplicity of each eigenvalue equals its algebraic multiplicity.
}

\textBox{%
The eigenvalue problem is one of the most fundamental in numerical linear algebra, with applications ranging from structural engineering (vibration analysis) to quantum mechanics (energy states), machine learning (principal component analysis), and dynamical systems (stability analysis). The efficiency and accuracy of eigenvalue algorithms have direct implications for these applications, making them a central focus in scientific computing.
}




\end{document}